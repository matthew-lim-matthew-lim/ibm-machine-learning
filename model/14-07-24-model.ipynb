{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://datasocibmproject.s3.ap-southeast-2.amazonaws.com/structured_data/capital_project_schedules_and_budgets_1.csv\")\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97227/965938000.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['final_estimate_of_actual_costs_through_end_of_phase_amount'].fillna(df['final_estimate_of_actual_costs_through_end_of_phase_amount'].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE: CLEAN DATA HERE\n",
    "df['final_estimate_of_actual_costs_through_end_of_phase_amount'].fillna(df['final_estimate_of_actual_costs_through_end_of_phase_amount'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6548, 15), (1637, 15))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['project_geographic_district', 'project_building_identifier',\n",
       "       'project_school_name', 'project_type', 'project_description',\n",
       "       'project_phase_name', 'project_status_name',\n",
       "       'project_phase_actual_start_date', 'project_phase_planned_end_date',\n",
       "       'project_phase_actual_end_date', 'project_budget_amount',\n",
       "       'final_estimate_of_actual_costs_through_end_of_phase_amount',\n",
       "       'total_phase_actual_spending_amount', 'dsf_number_s', 'failure'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE THE MODEL YOU WANT TO USE\n",
    "# model = LogisticRegression()\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "\"\"\"\n",
    "Generates predicted values for the 'failure' column in the test dataset\n",
    "\n",
    "Usage: predict(['col_a', 'col_b', 'col_c'])\n",
    "\"\"\"\n",
    "def predict(features):\n",
    "    model.fit(df_train[features], df_train['failure'])\n",
    "    y_pred = model.predict(df_test[features])\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Evaluates predictions against the true values in the test dataset\n",
    "\n",
    "Usage: eval(y_pred)\n",
    "\"\"\"\n",
    "def eval(y_pred):\n",
    "    y_actual = df_test['failure']\n",
    "\n",
    "    # Print number of predictions and percentage\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Predictions:\")\n",
    "    print(f\"failure=true: {int(np.sum(y_pred))}/{len(y_pred)} ({np.sum(y_pred) / len(y_pred):.2%})\")\n",
    "    print(f\"failure=false: {int(len(y_pred) - np.sum(y_pred))}/{len(y_pred)} ({1 - np.sum(y_pred) / len(y_pred):.2%})\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Accuracy: {np.mean(y_pred == y_actual):.5%}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_actual, y_pred)\n",
    "\n",
    "    # Extract values from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate totals\n",
    "    total_positives = tp + fn\n",
    "    total_negatives = tn + fp\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(f\"True positives: {tp}/{total_positives} ({tp / total_positives:.2%})\")\n",
    "    print(f\"True negatives: {tn}/{total_negatives} ({tn / total_negatives:.2%})\")\n",
    "    print(f\"False positives: {fp}/{total_negatives} ({fp / total_negatives:.2%})\")\n",
    "    print(f\"False negatives: {fn}/{total_positives} ({fn / total_positives:.2%})\")\n",
    "    print(\"-----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Predictions:\n",
      "failure=true: 0/1637 (0.00%)\n",
      "failure=false: 1637/1637 (100.00%)\n",
      "-----------------------------------------\n",
      "Accuracy: 84.91142%\n",
      "-----------------------------------------\n",
      "True positives: 0/247 (0.00%)\n",
      "True negatives: 1390/1390 (100.00%)\n",
      "False positives: 0/1390 (0.00%)\n",
      "False negatives: 247/247 (100.00%)\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Baseline, all no failure\n",
    "eval(np.zeros(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Predictions:\n",
      "failure=true: 223/1637 (13.62%)\n",
      "failure=false: 1414/1637 (86.38%)\n",
      "-----------------------------------------\n",
      "Accuracy: 86.07208%\n",
      "-----------------------------------------\n",
      "True positives: 121/247 (48.99%)\n",
      "True negatives: 1288/1390 (92.66%)\n",
      "False positives: 102/1390 (7.34%)\n",
      "False negatives: 126/247 (51.01%)\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE \n",
    "y_pred = predict(['final_estimate_of_actual_costs_through_end_of_phase_amount'])\n",
    "eval(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
